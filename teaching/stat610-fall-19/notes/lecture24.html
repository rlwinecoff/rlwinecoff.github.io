<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>lecture24</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" type="text/css" media="screen, projection, print"
    href="https://www.w3.org/Talks/Tools/Slidy2/styles/slidy.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <script src="https://www.w3.org/Talks/Tools/Slidy2/scripts/slidy.js"
    charset="utf-8" type="text/javascript"></script>
</head>
<body>
<div id="monte-carlo-methods-integration-and-cross-validation" class="slide section level2">
<h1>Monte Carlo methods: integration and cross validation</h1>
<p>Agenda today: Simulation-based methods of computing expectations</p>
<ul>
<li><p>Monte Carlo integration</p></li>
<li><p>Cross validation</p></li>
</ul>
<p>Reading: Lange 21.1, 21.2, 21.6</p>
</div>
<div id="monte-carlo-integration" class="slide section level2">
<h1>Monte Carlo Integration</h1>
<p>We have:</p>
<ul>
<li><p>A function <span class="math inline">\(f\)</span></p></li>
<li><p>A distribution <span class="math inline">\(\mu\)</span></p></li>
</ul>
<p>We would like to approximate <span class="math display">\[
E[f(X)] = \int f(x) \, d\mu(x)
\]</span></p>
<div class="incremental">
<p>Why not numerical integration?</p>
<ul>
<li><p>Works well for low-dimensional problems.</p></li>
<li><p>Fails in high dimensions, the “curse of dimensionality”</p></li>
</ul>
</div>
</div>
<div id="monte-carlo-integration-1" class="slide section level2">
<h1>Monte Carlo Integration</h1>
<p>To estimate/approximate <span class="math display">\[
E[f(X)] = \int f(x) \, d\mu(x)
\]</span></p>
<ul>
<li><p>Draw <span class="math inline">\(X_1, \ldots, X_n\)</span> iid from <span class="math inline">\(\mu\)</span></p></li>
<li><p>Use <span class="math inline">\(\frac{1}{n} \sum_{i=1}^n f(X_i)\)</span> as the estimator</p></li>
</ul>
<div class="incremental">
<p>Why is this reasonable?</p>
<ul>
<li>By the law of large numbers, the estimates converge to <span class="math inline">\(E[f(X)]\)</span> as <span class="math inline">\(n \to \infty\)</span></li>
</ul>
</div>
</div>
<div id="monte-carlo-integration-accuracy" class="slide section level2">
<h1>Monte Carlo Integration: Accuracy</h1>
<p>If <span class="math inline">\(f\)</span> is square integrable, we can apply the central limit theorem, which tells us that the estimator is approximately distributed <span class="math display">\[
\mathcal N \left( E[f(X)], \sqrt{\frac{\text{Var}[f(X)]}{n}} \right)
\]</span></p>
<div class="incremental">
<ul>
<li><p>Good thing: accuracy doesn’t depend on the dimensionality of the problem</p></li>
<li><p>Bad thing: Error declines at the <span class="math inline">\(n^{-1/2}\)</span> rate vs. <span class="math inline">\(n^{-k}\)</span>, <span class="math inline">\(k \ge 2\)</span> for the numerical integration methods.</p></li>
<li><p>What to do? Try to decrease <span class="math inline">\(\text{Var}[f(X)]\)</span>.</p></li>
</ul>
</div>
</div>
<div id="importance-sampling" class="slide section level2">
<h1>Importance Sampling</h1>
<p>Importance sampling is based on the following equality: <span class="math display">\[
\int f(x) g(x)\, d \nu (x) = \int \frac{f(x) g(x)}{h(x)} h(x) \,d \nu(x)
\]</span> where</p>
<ul>
<li><p><span class="math inline">\(f\)</span> is the function for which we would like to compute the expectation</p></li>
<li><p><span class="math inline">\(\nu\)</span> is some base measure (Lebesgue or counting)</p></li>
<li><p><span class="math inline">\(g\)</span> is the density of our target probability distribution relative to <span class="math inline">\(\nu\)</span> (so the measure <span class="math inline">\(\mu\)</span> from before is <span class="math inline">\(g d \nu\)</span>).</p></li>
<li><p><span class="math inline">\(h\)</span> is the density of some other probability distribution relative to <span class="math inline">\(\nu\)</span></p></li>
</ul>
</div>
<div id="importance-sampling-procedure" class="slide section level2">
<h1>Importance sampling: Procedure</h1>
<p>Draw <span class="math inline">\(Y_1, \ldots, Y_n\)</span> iid from a distribution <span class="math inline">\(h\)</span>. Then <span class="math display">\[
\frac{1}{n}\sum_{i=1}^n \frac{f(Y_i) g(Y_i)}{h(Y_i)}
\]</span> is an estimate of <span class="math inline">\(\int f(x) g(x) \, d \nu(x)\)</span>.</p>
</div>
<div id="when-is-this-useful" class="slide section level2">
<h1>When is this useful?</h1>
<p>The importance sampling estimator has a smaller variance than the naive estimator iff: <span class="math display">\[
\int \left[ \frac{f(x) g(x)}{h(x)} \right]^2 h(x) d\nu(x) \le \int f(x)^2 g(x) d\nu(x)
\]</span></p>
<div class="incremental">
<ul>
<li>If we choose <span class="math inline">\(h(x) = |f(x)| g(x) / \int |f(z)| g(z) d\nu(z)\)</span>, then an application of Cauchy Schwarz tells us that the condition above is satisfied and the importance sampling estimator will have smaller variance.</li>
</ul>
</div>
<div class="incremental">
<ul>
<li>If <span class="math inline">\(f\)</span> is nonnegative and <span class="math inline">\(h\)</span> is chosen as above, the variance of the importance sampling estimator is 0.</li>
</ul>
</div>
<div class="incremental">
<ul>
<li>We aren’t able to choose <span class="math inline">\(h\)</span> according to the recipe above, but it implies that the variance will be reduced if <span class="math inline">\(h(x)\)</span> looks like <span class="math inline">\(|f(x)| g(x)\)</span>.</li>
</ul>
</div>
</div>
<div id="a-contrived-example" class="slide section level2">
<h1>A contrived example</h1>
<p>We are playing a terrible game:</p>
<ul>
<li><p>I draw from a uniform distribution on <span class="math inline">\([0,1]\)</span>.</p></li>
<li><p>If the draw comes up less than <span class="math inline">\(.01\)</span>, you have to pay me $100.</p></li>
<li><p>Otherwise nothing happens.</p></li>
</ul>
<p>What is your expected return to playing this game?</p>
</div>
<div class="slide section level2">

<p>Naive Monte Carlo estimate:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb1-2"><a href="#cb1-2"></a>f =<span class="st"> </span><span class="cf">function</span>(x) {</span>
<span id="cb1-3"><a href="#cb1-3"></a>    <span class="cf">if</span>(x <span class="op">&lt;</span><span class="st"> </span><span class="fl">.01</span>)</span>
<span id="cb1-4"><a href="#cb1-4"></a>        <span class="kw">return</span>(<span class="op">-</span><span class="dv">100</span>)</span>
<span id="cb1-5"><a href="#cb1-5"></a>    <span class="kw">return</span>(<span class="dv">0</span>)</span>
<span id="cb1-6"><a href="#cb1-6"></a>}</span>
<span id="cb1-7"><a href="#cb1-7"></a>mc_samples =<span class="st"> </span><span class="kw">runif</span>(<span class="dv">1000</span>)</span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="kw">mean</span>(<span class="kw">sapply</span>(mc_samples, f))</span></code></pre></div>
<pre><code>## [1] -0.7</code></pre>
<ul>
<li><p>Not very good!</p></li>
<li><p>Problem is that we don’t have very many samples where <span class="math inline">\(x &lt; .01\)</span></p></li>
<li><p>Try importance sampling from a distribution that is more likely to give <span class="math inline">\(x &lt; .01\)</span></p></li>
</ul>
</div>
<div class="slide section level2">

<p>Recall Beta distributions: Supported on the interval <span class="math inline">\([0,1]\)</span>, can tune so that they put more weight in the middle or at the edges.</p>
<p>Beta(1,10) has density:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1"></a>x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">length.out =</span> <span class="dv">200</span>)</span>
<span id="cb3-2"><a href="#cb3-2"></a>beta_density =<span class="st"> </span><span class="kw">dbeta</span>(x, <span class="dt">shape1 =</span> <span class="dv">1</span>, <span class="dt">shape2 =</span> <span class="dv">10</span>)</span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="kw">plot</span>(beta_density <span class="op">~</span><span class="st"> </span>x, <span class="dt">type =</span> <span class="st">&#39;l&#39;</span>)</span></code></pre></div>
<p><img src="lecture-24-fig/unnamed-chunk-2-1.png" /></p>
</div>
<div class="slide section level2">

<p>Importance sampling from Beta(1,10):</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1"></a>mc_importance_samples =<span class="st"> </span><span class="kw">rbeta</span>(<span class="dv">1000</span>, <span class="dt">shape1 =</span> <span class="dv">1</span>, <span class="dt">shape2 =</span> <span class="dv">10</span>)</span>
<span id="cb4-2"><a href="#cb4-2"></a>importance_function =<span class="st"> </span><span class="cf">function</span>(x) {</span>
<span id="cb4-3"><a href="#cb4-3"></a>    <span class="kw">return</span>(<span class="kw">f</span>(x) <span class="op">/</span><span class="st"> </span><span class="kw">dbeta</span>(x, <span class="dt">shape1 =</span> <span class="dv">1</span>, <span class="dt">shape2 =</span> <span class="dv">10</span>))</span>
<span id="cb4-4"><a href="#cb4-4"></a>}</span>
<span id="cb4-5"><a href="#cb4-5"></a><span class="kw">mean</span>(<span class="kw">sapply</span>(mc_importance_samples, importance_function))</span></code></pre></div>
<pre><code>## [1] -1.044691</code></pre>
</div>
<div id="more-realistic-examples" class="slide section level2">
<h1>More realistic examples</h1>
<ul>
<li><p>Intuition from the game holds: if you have extreme returns from rare events, importance sampling by sampling more from regions with extreme returns helps</p></li>
<li><p>Insurance</p></li>
<li><p>Quantitative finance</p></li>
</ul>
</div>
<div id="rao-blackwellization" class="slide section level2">
<h1>Rao-Blackwellization</h1>
<p>Let <span class="math inline">\(\tau(U, X)\)</span> be an estimator of some function</p>
<p>Then <span class="math display">\[
\text{var}(\tau(U, X)) = \text{var}[E[\tau(U, X) | X]] + E[\text{var}(\tau(U, X) | X)]
\]</span> which means that <span class="math inline">\(\tau&#39; = E[\tau(U, X) \mid X]\)</span> has a smaller variance than <span class="math inline">\(\tau\)</span></p>
</div>
<div id="rao-blackwellization-for-monte-carlo-integration-using-the-acceptreject-algorithm" class="slide section level2">
<h1>“Rao-Blackwellization” for Monte Carlo Integration using the Accept/Reject algorithm</h1>
<p>Setup:</p>
<ul>
<li><p>We want to estimate <span class="math inline">\(E[f(X)] = \int f(x) g(x) \, dx\)</span></p></li>
<li><p>We have some <span class="math inline">\(h\)</span>, <span class="math inline">\(c\)</span> such that <span class="math inline">\(g(x) \le ch(x)\)</span></p></li>
<li><p>We generate <span class="math inline">\(N\)</span> points <span class="math inline">\(X_1,\ldots, X_N\)</span> from <span class="math inline">\(h(x)\)</span></p></li>
<li><p>We generate <span class="math inline">\(N\)</span> points <span class="math inline">\(U_1,\ldots, U_N\)</span> from a standard uniform.</p></li>
<li><p>We accept <span class="math inline">\(X_i\)</span> if <span class="math inline">\(U_i \le W_i = g(X_i) / [ch(X_i)]\)</span>.</p></li>
<li><p><span class="math inline">\(N\)</span> is a random number so that we have <span class="math inline">\(m\)</span> acceptances.</p></li>
</ul>
<p>The Monte Carlo estimator of <span class="math inline">\(E[f(X)]\)</span> is <span class="math display">\[
\frac{1}{m} \sum_{i=1}^N \mathbf 1_{\{U_i \le W_i\}} f(X_i)
\]</span></p>
</div>
<div class="slide section level2">

<p>Can we condition?</p>
<p><span class="math display">\[
\begin{align*}
\frac{1}{m} E&amp;\left[ \sum_{i=1}^N \mathbf 1_{\{U_i \le W_i\}} f(X_i) | N, X_1,\ldots, X_N \right] \\
&amp;= \frac{1}{m}\sum_{i=1}^N E\left[ \mathbf 1_{\{U_i \le W_i\}}  | N, W_1,\ldots, W_N\right] f(X_i)
\end{align*}
\]</span></p>
<p><span class="math inline">\(E \left[ \mathbf 1_{\{U_i \le W_i\}} | N, W_1,\ldots, W_N \right]\)</span> is the probability that we accept <span class="math inline">\(X_i\)</span> given that we sample <span class="math inline">\(N\)</span> deviates and accept <span class="math inline">\(m\)</span> of them</p>
<p>If <span class="math inline">\(m\)</span>, <span class="math inline">\(N\)</span> are large, we will have</p>
<p><span class="math display">\[
E \left[ \mathbf 1_{\{U_i \le W_i\}} | N, W_1,\ldots, W_N \right] \approx W_i m / N \approx W_i / c
\]</span></p>
</div>
<div class="slide section level2">

<p>Example:</p>
<p><span class="math inline">\(h\)</span> the standard uniform distribution, <span class="math inline">\(c = 10\)</span></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1"></a>one_rb_sample =<span class="st"> </span><span class="cf">function</span>() {</span>
<span id="cb6-2"><a href="#cb6-2"></a>    c =<span class="st"> </span><span class="dv">10</span></span>
<span id="cb6-3"><a href="#cb6-3"></a>    X =<span class="st"> </span><span class="kw">runif</span>(<span class="dv">1</span>)</span>
<span id="cb6-4"><a href="#cb6-4"></a>    U =<span class="st"> </span><span class="kw">runif</span>(<span class="dv">1</span>)</span>
<span id="cb6-5"><a href="#cb6-5"></a>    W =<span class="st"> </span><span class="kw">dbeta</span>(X, <span class="dt">shape1 =</span> <span class="dv">1</span>, <span class="dt">shape2 =</span> <span class="dv">10</span>) <span class="op">/</span><span class="st"> </span>(c <span class="op">*</span><span class="st"> </span><span class="kw">dunif</span>(X))</span>
<span id="cb6-6"><a href="#cb6-6"></a>    <span class="kw">return</span>(<span class="kw">c</span>(<span class="dt">accepted =</span> (U <span class="op">&lt;=</span><span class="st"> </span>W), <span class="dt">fX =</span> <span class="kw">f</span>(X), <span class="dt">weight =</span> W <span class="op">/</span><span class="st"> </span>c))</span>
<span id="cb6-7"><a href="#cb6-7"></a>}</span>
<span id="cb6-8"><a href="#cb6-8"></a>samples =<span class="st"> </span><span class="kw">replicate</span>(<span class="dt">n =</span> <span class="dv">10000</span>, <span class="kw">one_rb_sample</span>())</span>
<span id="cb6-9"><a href="#cb6-9"></a>m =<span class="st"> </span><span class="kw">sum</span>(samples[<span class="st">&quot;accepted&quot;</span>,])</span>
<span id="cb6-10"><a href="#cb6-10"></a>fX =<span class="st"> </span>samples[<span class="st">&quot;fX&quot;</span>,]</span>
<span id="cb6-11"><a href="#cb6-11"></a>weights =<span class="st"> </span>samples[<span class="st">&quot;weight&quot;</span>,]</span>
<span id="cb6-12"><a href="#cb6-12"></a><span class="kw">sum</span>(fX <span class="op">*</span><span class="st"> </span>weights) <span class="op">/</span><span class="st"> </span>m</span></code></pre></div>
<pre><code>## [1] -0.9477492</code></pre>
</div>
<div id="part-2-cross-validation" class="slide section level2">
<h1>Part 2: Cross validation</h1>
<div class="incremental">
<p>We have:</p>
<ul>
<li><p>Data <span class="math inline">\(X_1, \ldots, X_n\)</span>.</p></li>
<li><p>A tuning parameter <span class="math inline">\(\theta\)</span>. Each value of <span class="math inline">\(\theta\)</span> corresponds to a different set of models.</p></li>
<li><p>A function <span class="math inline">\(L\)</span> that takes a fitted model and a data point and returns a measure of model quality.</p></li>
</ul>
<p>We would like to choose one model from the set of candidate models indexed by <span class="math inline">\(\theta\)</span>.</p>
</div>
</div>
<div id="example-regression" class="slide section level2">
<h1>Example: Regression</h1>
<ul>
<li><p>Data: Pairs of predictors and response variables, <span class="math inline">\((y_i, X_i)\)</span>, <span class="math inline">\(i = 1,\ldots, n\)</span>, <span class="math inline">\(y_i \in \mathbb R\)</span>, <span class="math inline">\(X_i \in \mathbb R^p\)</span></p></li>
<li><p>Models: <span class="math inline">\(y_i = X \beta + \epsilon\)</span>, <span class="math inline">\(\beta_j = 0, j \in S_\theta\)</span>, where <span class="math inline">\(S_\theta \subseteq \{1,\ldots, p\}\)</span>.</p></li>
<li><p>Model quality: Squared-error loss. If <span class="math inline">\(\hat \beta_\theta\)</span> are our estimates of the regression coefficients in model <span class="math inline">\(\theta\)</span>, model quality is measured by <span class="math display">\[
L(\hat \beta_\theta, (y_i, X_i)) = (y_i - X_i^T \hat \beta_\theta)^2
\]</span></p></li>
</ul>
<p>We want to choose a subset of the predictors that do the best job of explaining the response.</p>
<div class="incremental">
<p>Naive solution: Find the model that has the lowest value for the squared-error loss.</p>
<p>Why doesn’t this work?</p>
</div>
</div>
<div id="example-mixture-models" class="slide section level2">
<h1>Example: Mixture models</h1>
<ul>
<li><p>Data: <span class="math inline">\(x_1,\ldots, x_n\)</span>, <span class="math inline">\(x_i \in \mathbb R\)</span></p></li>
<li><p>Models: Gaussian mixture models with <span class="math inline">\(\theta\)</span> mixture components.</p></li>
<li><p>Model quality: Negative log likelihood of the data. If <span class="math inline">\(\hat p_\theta\)</span> is the density of the fitted model with <span class="math inline">\(\theta\)</span> components, model quality is measured by <span class="math inline">\(L(\hat p_\theta, x_i) = -\log \hat p_\theta(x_i)\)</span>.</p></li>
</ul>
<p>We want to choose the number of mixture components that best explains the data.</p>
<div class="incremental">
<p>Naive solution: Choose the number of mixture components that minimizes the negative log likelihood of the data.</p>
</div>
</div>
<div id="better-solution-cross-validation" class="slide section level2">
<h1>Better Solution: Cross validation</h1>
<p>Idea: Instead of measuring model quality on the same data we used to fit the model, we estimate model quality on new data.</p>
<p>If we knew the true distribution of the data, we could simulate new data and use a Monte Carlo estimate based on the simulations.</p>
<p>We can’t actually get new data, and so we hold some back when we fit the model and then pretend that the held back data is new data.</p>
</div>
<div class="slide section level2">

<p>Procedure:</p>
<ul>
<li><p>Divide the data into <span class="math inline">\(K\)</span> folds</p></li>
<li><p>Let <span class="math inline">\(X^{(k)}\)</span> denote the data in fold <span class="math inline">\(k\)</span>, and let <span class="math inline">\(X^{(-k)}\)</span> denote the data in all the folds except for <span class="math inline">\(k\)</span>.</p></li>
<li><p>For each fold and each value of the tuning parameter <span class="math inline">\(\theta\)</span>, fit the model on <span class="math inline">\(X^{(-k)}\)</span> to get <span class="math inline">\(\hat f_\theta^{(k)}\)</span></p></li>
<li><p>Compute <span class="math display">\[
\text{CV}(\theta) = \frac{1}{n} \sum_{k=1}^K \sum_{x \in X^{(k)}} L(\hat f_\theta^{(k)}, x)
\]</span></p></li>
<li><p>Choose <span class="math inline">\(\hat \theta = \text{argmin}_{\theta} \text{CV}(\theta)\)</span></p></li>
</ul>
</div>
<div id="example" class="slide section level2">
<h1>Example</h1>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1"></a>n =<span class="st"> </span><span class="dv">100</span></span>
<span id="cb8-2"><a href="#cb8-2"></a>p =<span class="st"> </span><span class="dv">20</span></span>
<span id="cb8-3"><a href="#cb8-3"></a>X =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(n <span class="op">*</span><span class="st"> </span>p), <span class="dt">nrow =</span> n)</span>
<span id="cb8-4"><a href="#cb8-4"></a>y =<span class="st"> </span><span class="kw">rnorm</span>(n)</span>
<span id="cb8-5"><a href="#cb8-5"></a>get_rss_submodels =<span class="st"> </span><span class="cf">function</span>(n_predictors, y, X) {</span>
<span id="cb8-6"><a href="#cb8-6"></a>    <span class="cf">if</span>(n_predictors <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) {</span>
<span id="cb8-7"><a href="#cb8-7"></a>        lm_submodel =<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="dv">0</span>)</span>
<span id="cb8-8"><a href="#cb8-8"></a>    } <span class="cf">else</span> {</span>
<span id="cb8-9"><a href="#cb8-9"></a>        lm_submodel =<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span>X[,<span class="dv">1</span><span class="op">:</span>n_predictors, <span class="dt">drop =</span> <span class="ot">FALSE</span>])</span>
<span id="cb8-10"><a href="#cb8-10"></a>    }</span>
<span id="cb8-11"><a href="#cb8-11"></a>    <span class="kw">return</span>(<span class="kw">sum</span>(<span class="kw">residuals</span>(lm_submodel)<span class="op">^</span><span class="dv">2</span>))</span>
<span id="cb8-12"><a href="#cb8-12"></a>}</span>
<span id="cb8-13"><a href="#cb8-13"></a>p_vec =<span class="st"> </span><span class="dv">0</span><span class="op">:</span>p</span>
<span id="cb8-14"><a href="#cb8-14"></a>rss =<span class="st"> </span><span class="kw">sapply</span>(p_vec, get_rss_submodels, y, X)</span>
<span id="cb8-15"><a href="#cb8-15"></a><span class="kw">plot</span>(rss <span class="op">~</span><span class="st"> </span>p_vec)</span></code></pre></div>
<p><img src="lecture-24-fig/unnamed-chunk-5-1.png" /></p>
</div>
<div class="slide section level2">

<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1"></a>get_cv_error =<span class="st"> </span><span class="cf">function</span>(n_predictors, y, X, folds) {</span>
<span id="cb9-2"><a href="#cb9-2"></a>    cv_vec =<span class="st"> </span><span class="kw">numeric</span>(<span class="kw">length</span>(<span class="kw">unique</span>(folds)))</span>
<span id="cb9-3"><a href="#cb9-3"></a>    <span class="cf">for</span>(f <span class="cf">in</span> <span class="kw">unique</span>(folds)) {</span>
<span id="cb9-4"><a href="#cb9-4"></a>        cv_vec[f] =<span class="st"> </span><span class="kw">rss_on_held_out</span>(</span>
<span id="cb9-5"><a href="#cb9-5"></a>                  n_predictors,</span>
<span id="cb9-6"><a href="#cb9-6"></a>                  <span class="dt">y_train =</span> y[folds <span class="op">!=</span><span class="st"> </span>f],</span>
<span id="cb9-7"><a href="#cb9-7"></a>                  <span class="dt">X_train =</span> X[folds <span class="op">!=</span><span class="st"> </span>f,],</span>
<span id="cb9-8"><a href="#cb9-8"></a>                  <span class="dt">y_test =</span> y[folds <span class="op">==</span><span class="st"> </span>f],</span>
<span id="cb9-9"><a href="#cb9-9"></a>                  <span class="dt">X_test =</span> X[folds <span class="op">==</span><span class="st"> </span>f,])</span>
<span id="cb9-10"><a href="#cb9-10"></a>    }</span>
<span id="cb9-11"><a href="#cb9-11"></a>    <span class="kw">return</span>(<span class="kw">mean</span>(cv_vec))</span>
<span id="cb9-12"><a href="#cb9-12"></a>}</span>
<span id="cb9-13"><a href="#cb9-13"></a></span>
<span id="cb9-14"><a href="#cb9-14"></a>rss_on_held_out =<span class="st"> </span><span class="cf">function</span>(n_predictors, y_train, X_train, y_test, X_test) {</span>
<span id="cb9-15"><a href="#cb9-15"></a>    <span class="cf">if</span>(n_predictors <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) {</span>
<span id="cb9-16"><a href="#cb9-16"></a>        lm_submodel =<span class="st"> </span><span class="kw">lm</span>(y_train <span class="op">~</span><span class="st"> </span><span class="dv">0</span>)</span>
<span id="cb9-17"><a href="#cb9-17"></a>        preds_on_test =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">length</span>(y_test))</span>
<span id="cb9-18"><a href="#cb9-18"></a>    } <span class="cf">else</span> {</span>
<span id="cb9-19"><a href="#cb9-19"></a>        lm_submodel =<span class="st"> </span><span class="kw">lm</span>(y_train <span class="op">~</span><span class="st"> </span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span>X_train[,<span class="dv">1</span><span class="op">:</span>n_predictors, <span class="dt">drop =</span> <span class="ot">FALSE</span>])</span>
<span id="cb9-20"><a href="#cb9-20"></a>        preds_on_test =<span class="st"> </span>X_test[,<span class="dv">1</span><span class="op">:</span>n_predictors, drop=<span class="st"> </span><span class="ot">FALSE</span>] <span class="op">%*%</span><span class="st"> </span><span class="kw">coef</span>(lm_submodel)</span>
<span id="cb9-21"><a href="#cb9-21"></a>    }</span>
<span id="cb9-22"><a href="#cb9-22"></a></span>
<span id="cb9-23"><a href="#cb9-23"></a>    <span class="kw">return</span>(<span class="kw">sum</span>((y_test <span class="op">-</span><span class="st"> </span>preds_on_test)<span class="op">^</span><span class="dv">2</span>))</span>
<span id="cb9-24"><a href="#cb9-24"></a>}</span>
<span id="cb9-25"><a href="#cb9-25"></a>K =<span class="st"> </span><span class="dv">5</span></span>
<span id="cb9-26"><a href="#cb9-26"></a><span class="co">## normally you would do this at random</span></span>
<span id="cb9-27"><a href="#cb9-27"></a>folds =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>K, <span class="dt">each =</span> n <span class="op">/</span><span class="st"> </span>K)</span>
<span id="cb9-28"><a href="#cb9-28"></a>p_vec =<span class="st"> </span><span class="dv">0</span><span class="op">:</span>p</span>
<span id="cb9-29"><a href="#cb9-29"></a>cv_errors =<span class="st"> </span><span class="kw">sapply</span>(p_vec, get_cv_error, y, X, folds)</span>
<span id="cb9-30"><a href="#cb9-30"></a><span class="kw">plot</span>(cv_errors <span class="op">~</span><span class="st"> </span>p_vec)</span></code></pre></div>
<p><img src="lecture-24-fig/unnamed-chunk-6-1.png" /></p>
</div>
<div id="choice-of-k" class="slide section level2">
<h1>Choice of <span class="math inline">\(K\)</span></h1>
<p>Considerations:</p>
<ul>
<li><p>Larger <span class="math inline">\(K\)</span> means more computation (although sometimes there is a shortcut for leave-one-out cross validation)</p></li>
<li><p>Larger <span class="math inline">\(K\)</span> means less bias in the estimate of model accuracy</p></li>
<li><p>Larger <span class="math inline">\(K\)</span> also means more variance in the estimate, so we don’t necessarily want <span class="math inline">\(K = n\)</span></p></li>
<li><p>Usually choose <span class="math inline">\(K = 5\)</span> or <span class="math inline">\(K = 10\)</span></p></li>
<li><p>If your problem is structured (e.g. time series, spatial), you should choose the folds to respect the structure.</p></li>
</ul>
</div>
<div id="summing-up" class="slide section level2">
<h1>Summing up</h1>
<ul>
<li><p>We can use simulations to estimate arbitrary functions of our random variables.</p></li>
<li><p>If we know the underlying distribution, we can simply simulate from it (Monte Carlo integration).</p></li>
<li><p>If we don’t know the underlying distribution, we can “simulate” from the data by resampling from the data (cross validation). Resampling methods will do well to the extent that the observed data reflect the true data-generating distribution.</p></li>
</ul>
</div>
</body>
</html>
