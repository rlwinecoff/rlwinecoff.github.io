<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; }
code > span.dt { color: #902000; }
code > span.dv { color: #40a070; }
code > span.bn { color: #40a070; }
code > span.fl { color: #40a070; }
code > span.ch { color: #4070a0; }
code > span.st { color: #4070a0; }
code > span.co { color: #60a0b0; font-style: italic; }
code > span.ot { color: #007020; }
code > span.al { color: #ff0000; font-weight: bold; }
code > span.fu { color: #06287e; }
code > span.er { color: #ff0000; font-weight: bold; }
  </style>
  <link rel="stylesheet" type="text/css" media="screen, projection, print"
    href="http://www.w3.org/Talks/Tools/Slidy2/styles/slidy.css" />
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script src="http://www.w3.org/Talks/Tools/Slidy2/scripts/slidy.js"
    charset="utf-8" type="text/javascript"></script>
</head>
<body>
<div id="newtons-method" class="slide section level2">
<h1>Newton's method</h1>
<p>Agenda today</p>
<ul>
<li><p>Review maximum likelihood</p></li>
<li><p>Newton's method</p></li>
<li><p>Review method of moments</p></li>
</ul>
<p>Reading:</p>
<ul>
<li>Kenneth Lange, Numerical Analysis for Statisticians, Section 11.2, 13.3</li>
</ul>
<p>Assignments:</p>
<ul>
<li><p>Homework posted on the website, due after spring break</p></li>
<li><p>Final project descriptions up on the website, email me by Friday with the project you would like to do and your group.</p></li>
</ul>
</div>
<div id="maximum-likelihood" class="slide section level2">
<h1>Maximum likelihood</h1>
<p>Problem: We have a family of probability distributions, indexed by a parameter <span class="math">\(\theta\)</span>, and we need to choose one to describe the data.</p>
<p>Solution, heuristically:</p>
<ul>
<li><p>Assume that our data <span class="math">\(x_1, \ldots, x_n\)</span> are realizations of independent random variables <span class="math">\(X_1, \ldots, X_n\)</span>, each coming from the same distribution with parameter <span class="math">\(\theta\)</span>.</p></li>
<li><p>Find the value of <span class="math">\(\theta\)</span> that makes the data most likely.</p></li>
<li><p>Use either the probability density (continuous random variables) or probability mass (discrete random variables) to describe how likely the data is for a given value of the parameter <span class="math">\(\theta\)</span>.</p></li>
</ul>
</div>
<div class="slide section level2">

<p>Formally:</p>
<ul>
<li><p>Let <span class="math">\(f(x; \theta)\)</span> be the probability density function or probability mass function of a random variable with drawn from a distribution with parameter <span class="math">\(\theta\)</span>.</p></li>
<li><p>With independent data points <span class="math">\(x_1\)</span>, <span class="math">\(x_2\)</span>, <span class="math">\(x_n\)</span>, the likelihood is</p></li>
</ul>
<p><span class="math">\[
L(\theta)=\prod_{i=1}^n f(x_i;\theta)
\]</span></p>
<div class="incremental">
<p>Recall that the probability density/mass function describes how likely a random variable is to take a given value.</p>
<ul>
<li><p>If <span class="math">\(f(x_i; \theta)\)</span> is high, it is very likely that we would see the value <span class="math">\(x_i\)</span> if <span class="math">\(x_i\)</span> really came from a distribution with parameter <span class="math">\(\theta\)</span></p></li>
<li><p>If <span class="math">\(f(x_i; \theta)\)</span> is low, it is unlikely that we would see the value <span class="math">\(x_i\)</span> if <span class="math">\(x_i\)</span> really came from a distribution with parameter <span class="math">\(\theta\)</span></p></li>
</ul>
<p>Therefore: find the value of <span class="math">\(\theta\)</span> that maximizes the likelihood.</p>
</div>
</div>
<div class="slide section level2">

<p>In practice, we work with the log likelihood instead of the likelihood:</p>
<p><span class="math">\[
\ell(\theta) = \sum_{i=1}^n \log f(x_i; \theta)
\]</span></p>
<ul>
<li><p>Easier to work with analytically</p></li>
<li><p>Better computationally because multiplying lots of small numbers together is bad (if you have a lot of data points you can get within machine tolerance of 0).</p></li>
</ul>
</div>
<div class="slide section level2">

<p>For example: we have data points <span class="math">\(x_1, \ldots, x_n\)</span>, and we want to find the <span class="math">\(N(\theta, 1)\)</span> distribution that fits the data the best.</p>
<div class="incremental">
<p>The likelihood is <span class="math">\[
L(x;  \theta) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}}\exp((x_i - \theta)^2)
\]</span></p>
<p>and the log likelihood is <span class="math">\[
\ell(x;  \theta) = \sum_{i=1}^n\log \left( \frac{1}{\sqrt{2\pi}}\exp((x_i - \theta)^2) \right)
\]</span></p>
</div>
</div>
<div class="slide section level2">

<p>We can use <code>dnorm</code> in R to compute the log likelihood for any <span class="math">\(x\)</span> and <span class="math">\(\theta\)</span> we want:</p>
<pre class="sourceCode r"><code class="sourceCode r">## create a function that computes the log likelihood
likelihood =<span class="st"> </span>function(theta, x) {
    <span class="kw">sum</span>(<span class="kw">log</span>(<span class="kw">dnorm</span>(x, <span class="dt">mean =</span> theta, <span class="dt">sd =</span> <span class="dv">1</span>)))
}
x =<span class="st"> </span><span class="kw">c</span>(<span class="fl">5.5</span>, <span class="dv">4</span>, <span class="fl">3.2</span>, <span class="fl">4.7</span>, <span class="fl">4.3</span>, <span class="fl">3.5</span>)
theta_vec =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dt">length.out =</span> <span class="dv">100</span>)
l_of_theta =<span class="st"> </span><span class="kw">sapply</span>(theta_vec, likelihood, x)
<span class="kw">plot</span>(l_of_theta ~<span class="st"> </span>theta_vec, <span class="dt">type =</span> <span class="st">&#39;l&#39;</span>)</code></pre>
<div class="figure">
<img src="lecture-15-fig/unnamed-chunk-1-1.png" />
</div>
</div>
<div class="slide section level2">

<p>What is the maximum?</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(l_of_theta ~<span class="st"> </span>theta_vec, <span class="dt">type =</span> <span class="st">&#39;l&#39;</span>)
<span class="kw">abline</span>(<span class="dt">v =</span> <span class="kw">mean</span>(x))</code></pre>
<div class="figure">
<img src="lecture-15-fig/unnamed-chunk-2-1.png" />
</div>
</div>
<div class="slide section level2">

<p>Alternately, just search over the grid:</p>
<pre class="sourceCode r"><code class="sourceCode r">max_idx =<span class="st"> </span><span class="kw">which.max</span>(l_of_theta)
theta_vec[max_idx]</code></pre>
<pre><code>## [1] 4.242424</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">## compare with
<span class="kw">mean</span>(x)</code></pre>
<pre><code>## [1] 4.2</code></pre>
</div>
<div class="slide section level2">

<p>Another example: Binomial, five trials, unknown success probability.</p>
<pre class="sourceCode r"><code class="sourceCode r">likelihood =<span class="st"> </span>function(theta, x) {
    <span class="kw">sum</span>(<span class="kw">log</span>(<span class="kw">dbinom</span>(<span class="dt">x =</span> x, <span class="dt">size =</span> <span class="dv">5</span>, <span class="dt">prob =</span> theta)))
}</code></pre>
</div>
<div class="slide section level2">

<p>Compute the likelihoods for many possible values of <code>prob</code></p>
<pre class="sourceCode r"><code class="sourceCode r">x =<span class="st"> </span><span class="kw">rbinom</span>(<span class="dt">n =</span> <span class="dv">100</span>, <span class="dt">size =</span> <span class="dv">5</span>, <span class="dt">prob =</span> .<span class="dv">2</span>)
theta_vec =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">length.out =</span> <span class="dv">100</span>)
log_likelihoods =<span class="st"> </span><span class="kw">sapply</span>(theta_vec, likelihood, x)
<span class="kw">plot</span>(log_likelihoods ~<span class="st"> </span>theta_vec, <span class="dt">type =</span> <span class="st">&#39;l&#39;</span>)
<span class="kw">abline</span>(<span class="dt">v =</span> .<span class="dv">2</span>)</code></pre>
<div class="figure">
<img src="lecture-15-fig/unnamed-chunk-5-1.png" />
</div>
</div>
<div class="slide section level2">

<p>We see that the maximum is pretty close to the true value, <span class="math">\(.2\)</span></p>
<pre class="sourceCode r"><code class="sourceCode r">max_idx =<span class="st"> </span><span class="kw">which.max</span>(log_likelihoods)
theta_vec[max_idx]</code></pre>
<pre><code>## [1] 0.2121212</code></pre>
</div>
<div id="newtons-method-1" class="slide section level2">
<h1>Newton's method</h1>
<ul>
<li><p>Iterative method for finding local minimum/maximum of a function.</p></li>
<li><p>Also known as Newton-Raphson, after Isaac Newton and Joseph Raphson (Raphson published in 1690, Newton wrote a similar method in 1671 but didn't publish until 1736)</p></li>
<li><p>Initial description is for finding zeros of a function</p></li>
<li><p>This turns out to be equivalent to an optimization problem/finding maxima of functions, as we need.</p></li>
</ul>
</div>
<div id="notation" class="slide section level2">
<h1>Notation</h1>
<ul>
<li><p><span class="math">\(\theta\)</span>: The parameter(s), either a scalar or a vector.</p></li>
<li><p><span class="math">\(\ell (\theta)\)</span>: The log likelihood at <span class="math">\(\theta\)</span>.</p></li>
<li><p><span class="math">\(d \ell(\theta)\)</span>: The first derivative of the log likelihood at <span class="math">\(\theta\)</span> (if <span class="math">\(\theta\)</span> is a scalar) or the vector of first derivatives of the log likelihood at <span class="math">\(\theta\)</span> (if <span class="math">\(\theta\)</span> is a vector).</p></li>
<li><p><span class="math">\(d^2 \ell(\theta)\)</span>: The second derivative of the log likelihood (if <span class="math">\(\theta\)</span> is a scalar) or the matrix of second partial derivatives (if <span class="math">\(\theta\)</span> is a vector.)</p></li>
</ul>
</div>
<div id="what" class="slide section level2">
<h1>What</h1>
<p>Our goal: Find the value of <span class="math">\(\theta\)</span> that maximizes <span class="math">\(\ell(\theta)\)</span>.</p>
<div class="incremental">
<p>Given that we are at a point <span class="math">\(\theta_n\)</span>, one Newton step is given by</p>
<p><span class="math">\[
\theta_{n+1} = \theta_n - d^2 \ell(\theta_n)^{-1} d \ell(\theta_n)
\]</span></p>
</div>
<div class="incremental">
<p>Newton's method algorithm:</p>
<ul>
<li><p>Start at a point <span class="math">\(\theta_0\)</span></p></li>
<li><p>Iterate <span class="math">\(\theta_{n+1} = \theta_n - d^2 \ell(\theta_n)^{-1} d \ell(\theta_n)\)</span> until some stopping criterion is reached.</p></li>
<li><p>Usually stop when the derivative is sufficiently close to zero.</p></li>
</ul>
</div>
<div class="incremental">
<p>Notice:</p>
<ul>
<li><p>Any stationary point of <span class="math">\(\ell(\theta)\)</span> is a fixed point of Newton's method.</p></li>
<li><p>Not necessarily an ascent algorithm</p></li>
</ul>
</div>
</div>
<div id="why" class="slide section level2">
<h1>Why</h1>
<p>Suppose we want to maximze a quadratic:</p>
<p><span class="math">\[
f(\theta) = a + b \theta + c \theta^2
\]</span></p>
<div class="incremental">
<p>We can solve for the maximum/minimum analytically by setting the first derivative equal to 0:</p>
<p><span class="math">\[
d f(\theta) = b + 2 c \theta
\]</span></p>
</div>
<div class="incremental">
<p>If we want <span class="math">\(b + 2c \theta = 0\)</span>, we take <span class="math">\(\theta = -\frac{b }{2c}\)</span></p>
</div>
</div>
<div class="slide section level2">

<p>Recast this result as a &quot;step&quot; instead of a single minimization:</p>
<ul>
<li><p>We start at <span class="math">\(\theta_0\)</span></p></li>
<li><p><span class="math">\(\theta_1\)</span> should be <span class="math">\(-\frac{b}{2c}\)</span></p></li>
<li><p>The step is <span class="math">\(\theta_1 - \theta_0\)</span>, which is <br><br> <span class="math">\[
\theta_1 - \theta_0 = -\frac{b}{2c} - \theta_0 \\
= -\frac{b + 2c \theta_0}{2c} \\
= -\frac{d f(\theta_0)}{d^2 f(\theta_0)}
\]</span> <br><br> since <span class="math">\(df(\theta_0) = b + 2c\theta_0\)</span> and <span class="math">\(d^2 f(\theta_0) = 2c\)</span></p></li>
</ul>
</div>
<div class="slide section level2">

<p>Intuition for general, not-necessarily-quadratic functions:</p>
<ul>
<li><p>We are not only dealing with quadratic functions, but we can approximate smooth, differentiable functions by quadratic functions.</p></li>
<li><p>Taylor approximation of <span class="math">\(\ell(\theta)\)</span> around <span class="math">\(\theta_0\)</span>: <span class="math">\[
\ell(\theta) \approx \ell(\theta_0) + d\ell(\theta_0)(\theta - \theta_0) + \frac{1}{2} (\theta - \theta_0)^T d^2 \ell(\theta_0) (\theta - \theta_0)
\]</span></p></li>
<li><p>A Newton step finds an extreme point for the approximation.</p></li>
</ul>
</div>
<div id="analysis" class="slide section level2">
<h1>Analysis</h1>
<p>Newton's method converges very rapidly close to the optimum.</p>
<p>We have the following analytic result:</p>
<p>Let <span class="math">\(\theta^\star\)</span> be the parameter corresponding to a local maximum.</p>
<p>If the second derivative matrix satisfies <span class="math">\[
\|d^2\ell(\phi) - d^2 \ell(\theta)\|\le \lambda \|\phi - \theta\|
\]</span> in some neighborhood of <span class="math">\(\theta^\star\)</span>, then the Newton iterates satisfy <span class="math">\[
\|\theta_{n+1} - \theta^\star \|\le 2 \lambda \|d^2 \ell(\theta^\star)^{-1} \| \|\theta_n - \theta^\star\|^2
\]</span></p>
<div class="incremental">
<p>Note:</p>
<ul>
<li><p>The condition on the second derivative matrix means that the curvature doesn't change much in a region around <span class="math">\(\theta^\star\)</span>, which you can think of as meaning <span class="math">\(\ell\)</span> is well approximated by a quadratic function.</p></li>
<li><p>Larger curvature at the optimum means faster convergence (remember <span class="math">\(-d^2 \ell(\theta)\)</span> is what statisticians call the information).</p></li>
</ul>
</div>
</div>
<div class="slide section level2">

<p>Potential issues:</p>
<ul>
<li><p><span class="math">\(d \ell\)</span>: Does it exist? Can we compute it?</p></li>
<li><p><span class="math">\(d^2\ell\)</span>: Does it exist? Can we compute it? Is it invertible?</p></li>
<li><p>How do we decide where to start?</p></li>
</ul>
</div>
<div id="method-of-moments" class="slide section level2">
<h1>Method of moments</h1>
<p>Same problem as maximum likelihood: we have a family of probability models, indexed by a scalar or vector <span class="math">\(\theta\)</span>, and we need to choose one to describe the data.</p>
<p>Idea:</p>
<ul>
<li><p>If <span class="math">\(\theta\)</span> is a <span class="math">\(k\)</span>-dimensional vector (we have <span class="math">\(k\)</span> parameters to estimate), derive expressions for the first <span class="math">\(k\)</span> moments of the data, <span class="math">\(E_\theta(X^r)\)</span>, <span class="math">\(r = 1,\ldots, k\)</span></p></li>
<li><p>Compute the first <span class="math">\(k\)</span> moments of the data: <br><br> <span class="math">\[
\frac{1}{n} \sum_{i=1}^n x_i^r, \quad r = 1,\ldots, k
\]</span></p></li>
<li><p><span class="math">\(\hat \theta\)</span> is the value of <span class="math">\(\theta\)</span> such that the empirical moments match the theoretical moments: <br><br> <span class="math">\[
E_{\hat \theta}(X^r) = \sum_{i=1}^n x_i^r, \quad r = 1,\ldots, k
\]</span></p></li>
</ul>
</div>
<div id="example-moment-estimator-for-normal-family" class="slide section level2">
<h1>Example: moment estimator for normal family</h1>
<p>Our family of distributions is <span class="math">\(N(\mu, \sigma^2)\)</span>, so that <span class="math">\(\theta = (\mu, \sigma)\)</span>.</p>
<div class="incremental">
<p>The first two moments are:</p>
<ul>
<li><p><span class="math">\(E_{\mu, \sigma}(X) = \mu\)</span></p></li>
<li><p><span class="math">\(E_{\mu, \sigma}(X^2) = \mu^2 + \sigma^2\)</span></p></li>
</ul>
</div>
</div>
<div class="slide section level2">

<p>Equate the first theoretical moment to the first data moment tells us that <span class="math">\(\hat \mu\)</span> should satisfy</p>
<p><span class="math">\[
E_{\hat \mu,\hat \sigma}(X) = \hat \mu = \frac{1}{n} \sum_{i=1}^n x_i
\]</span> and so <span class="math">\(\hat \mu = \frac{1}{n} \sum_{i=1}^n x_i\)</span></p>
<div class="incremental">
<p>Then equating the second theoretical moments to the second data moment tells us that <span class="math">\(\hat \mu\)</span> and <span class="math">\(\hat \sigma\)</span> should satisfy <span class="math">\[
E_{\mu, \sigma}(X^2) = \mu^2 + \sigma^2 = \frac{1}{n} \sum_{i=1}^n x_i^2
\]</span> Plugging in <span class="math">\(\hat \mu = \sum_{i=1}^n x_i\)</span> and solving gives us <span class="math">\[
E_{\hat \mu, \hat \sigma}(X^2) = \hat \mu^2 + \hat \sigma^2 \\
= (\frac{1}{n} \sum_{i=1}^n x_i)^2 + \hat \sigma^2\\
=\frac{1}{n}\sum_{i=1}^n x_i^2
\]</span> and so <span class="math">\(\hat \sigma^2 = \frac{1}{n}\sum_{i=1}^n x_i^2 - (\frac{1}{n}\sum_{i=1}^n x_i)^2\)</span>.</p>
<p>If you do a little more algebra, you can see that this is a standard estimate of the variance.</p>
</div>
</div>
<div id="example-moment-estimator-for-mixture-models" class="slide section level2">
<h1>Example: moment estimator for mixture models</h1>
<p>Mixture model</p>
<ul>
<li><p><span class="math">\(x_1, \ldots, x_n\)</span> come from a distribution with cumulative distribution function <span class="math">\(\theta G + (1 - \theta)H\)</span>, where <span class="math">\(G\)</span> and <span class="math">\(H\)</span> are two fixed, distributions (for example, two normal distributions with known means and variances, or two Poisson distributions with different means).</p></li>
<li><p>Let <span class="math">\(\xi\)</span> denote the mean of <span class="math">\(G\)</span> and <span class="math">\(\eta\)</span> denote the mean of <span class="math">\(H\)</span>.</p></li>
<li><p>We want to estimate the mixing parameter <span class="math">\(\theta\)</span>.</p></li>
</ul>
</div>
<div class="slide section level2">

<p>For example, we can visualize the density for a mixture of a <span class="math">\(N(0, .5)\)</span> and a <span class="math">\(N(3, 2.5)\)</span> distribution with mixing parameter <span class="math">\(\theta = .2\)</span>:</p>
<pre class="sourceCode r"><code class="sourceCode r">mean_G =<span class="st"> </span><span class="dv">0</span>
mean_H =<span class="st"> </span><span class="dv">3</span>
sd_G =<span class="st"> </span>.<span class="dv">5</span>
sd_H =<span class="st"> </span><span class="fl">2.5</span>
q_seq =<span class="st"> </span><span class="kw">seq</span>(-<span class="dv">3</span>, <span class="dv">8</span>, <span class="dt">length.out =</span> <span class="dv">1000</span>)
g =<span class="st"> </span><span class="kw">dnorm</span>(q_seq, <span class="dt">mean =</span> mean_G, <span class="dt">sd =</span> sd_G)
h =<span class="st"> </span><span class="kw">dnorm</span>(q_seq, <span class="dt">mean =</span> mean_H, <span class="dt">sd =</span> sd_H)
theta =<span class="st"> </span>.<span class="dv">2</span>
<span class="kw">plot</span>(theta *<span class="st"> </span>g +<span class="st"> </span>(<span class="dv">1</span> -<span class="st"> </span>theta) *<span class="st"> </span>h ~<span class="st"> </span>q_seq, <span class="dt">type =</span> <span class="st">&#39;l&#39;</span>)</code></pre>
<div class="figure">
<img src="lecture-15-fig/unnamed-chunk-7-1.png" />
</div>
</div>
<div class="slide section level2">

<p>We have one parameter, so we compute the first theoretical moment: <span class="math">\[
E_\theta(X) = \theta \xi + (1 - \theta) \eta
\]</span></p>
<div class="incremental">
<p>Then we equate that moment to the first data moment to get our estimate: <span class="math">\[
\hat \theta \xi + (1 - \hat \theta) \eta = \frac{1}{n} \sum_{i=1}^n x_i\\
\hat \theta = \frac{\frac{1}{n} \sum_{i=1}^n x_i - \eta}{\xi - \eta}
\]</span></p>
</div>
</div>
<div class="slide section level2">

<p>There isn't anything particularly important about the first <span class="math">\(k\)</span> moments, can match other aspects of the data</p>
<ul>
<li><p>Median</p></li>
<li><p>Other quantiles</p></li>
<li><p>Centered moments instead of raw moments</p></li>
</ul>
<p>We are thinking of these as starting values for maximum likelihood estimation, but they are usually reasonable estimators in their own right.</p>
<p>The idea of matching data statistics to expected values of statistics will come up again in approximate Bayesian computation.</p>
</div>
</body>
</html>
