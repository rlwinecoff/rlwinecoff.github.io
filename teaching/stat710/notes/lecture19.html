<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; }
code > span.dt { color: #902000; }
code > span.dv { color: #40a070; }
code > span.bn { color: #40a070; }
code > span.fl { color: #40a070; }
code > span.ch { color: #4070a0; }
code > span.st { color: #4070a0; }
code > span.co { color: #60a0b0; font-style: italic; }
code > span.ot { color: #007020; }
code > span.al { color: #ff0000; font-weight: bold; }
code > span.fu { color: #06287e; }
code > span.er { color: #ff0000; font-weight: bold; }
  </style>
  <link rel="stylesheet" type="text/css" media="screen, projection, print"
    href="http://www.w3.org/Talks/Tools/Slidy2/styles/slidy.css" />
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script src="http://www.w3.org/Talks/Tools/Slidy2/scripts/slidy.js"
    charset="utf-8" type="text/javascript"></script>
</head>
<body>
<div id="methods-for-constrained-optimization-problems" class="slide section level2">
<h1>Methods for constrained optimization problems</h1>
<p>Agenda for the week</p>
<ul>
<li><p>Today: methods for constrained optimization problems</p></li>
<li><p>Thursday: applications</p></li>
</ul>
</div>
<div id="the-problem-we-want-to-solve" class="slide section level2">
<h1>The problem we want to solve</h1>
<p><span class="math">\[
\begin{align*}
\text{minimize}_x \quad &amp;f(x)\\
\text{subject to} \quad &amp;f_1(x) = c
\end{align*}
\]</span></p>
</div>
<div id="for-example" class="slide section level2">
<h1>For example</h1>
<p>We roll a 6-sided die <span class="math">\(n\)</span> times. It comes up 1 <span class="math">\(n_1\)</span> times, 2 <span class="math">\(n_2\)</span> times, and so on up to <span class="math">\(n_6\)</span>.</p>
<p>We don't necessarily trust it to be fair, and so we want maximum likelihood estimates of the chance of rolling <span class="math">\(1, \ldots, 6\)</span>.</p>
<div class="incremental">
<p>We write down the likelihood: <span class="math">\[
L(p_1, \ldots, p_6) = \frac{n!}{\prod_{i=1}^6 n_i!} \prod_{i=1}^6 p_i^{n_i}
\]</span></p>
<p>And the log likelihood <span class="math">\[
\ell(p_1, \ldots, p_6) = \log \frac{n!}{\prod_{i=1}^6 n_i!} + \sum_{i=1}^6 n_i \log p_i
\]</span></p>
</div>
<div class="incremental">
<p>Optimize by taking the derivative and setting to zero? <span class="math">\[
\frac{\partial \ell}{\partial p_1} = \frac{n_1}{\theta_1} = 0
\]</span></p>
</div>
</div>
<div class="slide section level2">

<p>This is actually a constrained optimization problem, we don't just maximize the likelihood, we maximize subject to the constraint <span class="math">\(\sum_{i=1}^6 p_i = 1\)</span>.</p>
<p>Our problem really should be <span class="math">\[
\begin{align*}
\text{maximuze} \quad &amp;\ell(p_1, \ldots, p_6)\\
\text{subject to} \quad &amp;\sum_{i=1}^n  p_i= 1
\end{align*}
\]</span></p>
</div>
<div id="lagrange-multipliers" class="slide section level2">
<h1>Lagrange multipliers</h1>
<p>Suppose we want to minimize <span class="math">\(f(x)\)</span> subject to the constraint <span class="math">\(f_1(x) = c\)</span> (equivalently, <span class="math">\(f_1(x) - c = 0\)</span>).</p>
<p>We can form the <em>Lagrangian</em>:</p>
<p><span class="math">\[
\mathcal L(x, \lambda) = f(x) - \lambda (f_1(x) - c)
\]</span></p>
<p>If we minimize <span class="math">\(\mathcal L\)</span> over <span class="math">\(x\)</span> and <span class="math">\(\lambda\)</span>, the optimal value <span class="math">\(x^*\)</span> will be the same as the optimal value for the constrained problem.</p>
<div class="incremental">
<p>Why?</p>
<p><span class="math">\[
\begin{align*}
\nabla_x \mathcal L |_{x^*, \lambda^*} &amp;= \nabla f(x^*) - \lambda^* \nabla f_1(x^*) = 0 \\
\frac{\partial \mathcal L}{\partial \lambda}|_{x^*, \lambda^*} &amp;= f_1(x^*) - c = 0
\end{align*}
\]</span></p>
<ul>
<li>Second line tells us that <span class="math">\(f_1(x^*) - c = 0\)</span> at the optimal point.</li>
</ul>
</div>
<div class="incremental">
<ul>
<li><p>First line tells us that at the optimal point, the gradients of the objective function (<span class="math">\(f\)</span>) and the constraint function (<span class="math">\(f_1\)</span>) will be parallel.</p></li>
<li><p>Interpretation: At a point where gradients are not parallel, a tiny step along <span class="math">\(f_1\)</span> in one direction will lead to an increase in the value of <span class="math">\(f\)</span>, and a tiny step in the other direction will lead to a decrease in the value of <span class="math">\(f\)</span>. Therefore, that point can't be optimal.</p></li>
</ul>
<div class="figure">
<img src="LagrangeMultipliers2D.png" />
</div>
</div>
</div>
<div id="lagrange-multipliers-for-multinomial-likelihood" class="slide section level2">
<h1>Lagrange multipliers for multinomial likelihood</h1>
<p>Lagrangian for the constrained maximum likelihood problem:</p>
<p><span class="math">\[
\mathcal L(p_1, \ldots, p_6, \lambda) = \log \frac{n!}{\prod_{i=1}^6 n_i!} + \sum_{i=1}^6 n_i \log (p_i) - \lambda (\sum_{i=1}^6 p_i - 1)
\]</span></p>
<div class="incremental">
<p>Derivatives with respect to <span class="math">\(p_i\)</span>: <span class="math">\[
\frac{\partial \mathcal L}{\partial p_i}|_{p_i^*, \lambda^*} = \frac{n_i}{p_i^*} - \lambda^* = 0
\]</span> Rearranging tells us that <span class="math">\(\frac{n_i}{ \lambda^*} = p_i^*\)</span></p>
</div>
<div class="incremental">
<p>Derivative with respect to <span class="math">\(\lambda\)</span>: <span class="math">\[
\frac{\partial \mathcal L}{\partial \lambda}|_{p_i^*, \lambda^*} = \sum_{i=1}^6 p_i - 1 = 0
\]</span></p>
</div>
<div class="incremental">
<p>A bit more manipulation: <span class="math">\[
\sum_{i=1}^6 \frac{n_i}{\lambda^*} = \sum_{i=1}^6 p_i^* = 1
\]</span> so <span class="math">\(\lambda^* = \sum_{i=1}^6 n_i = n\)</span>, and <span class="math">\(p_i^* = \frac{n_i}{\sum_{i=1}^6 n_i}\)</span></p>
</div>
</div>
<div id="notes" class="slide section level2">
<h1>Notes</h1>
<ul>
<li><p>Useful because it makes equality-constrained problems into unconstrained problems, and we have a lot of ways of solving unconstrained problems</p></li>
<li><p>From the material on convex problems, we know that equality constraints are only likely to maintain the convexity of the problem if the equality constraints are linear/affine</p></li>
<li><p>But the theory goes through for non-linear equality constraints: if you can solve analytically you will get the right solution, and if you apply some of the descent methods we used for unconstrained problems to a non-convex problem you can still get a result</p></li>
<li><p>Can deal with multiple equality constraints by having multiple Lagrange multipliers.</p></li>
</ul>
</div>
<div id="inequality-constraints" class="slide section level2">
<h1>Inequality constraints</h1>
<p>Suppose we want to minimize <span class="math">\(f(x)\)</span> subject to the constraint <span class="math">\(f_1(x) \le c\)</span> (equivalently, <span class="math">\(f_1(x) - c \le 0\)</span>).</p>
<p>We could change it to an unconstrained problem by defining the following function: <span class="math">\[
I(x) = \begin{cases}
\infty &amp; f_1(x) - c &gt; 0 \\
0 &amp; f_1(x) - c \le 0
\end{cases}
\]</span></p>
<p>Then <span class="math">\[
\text{minimize } f(x) \quad \text{subject to } f_1(x) - c \le 0
\]</span> is equivalent to <span class="math">\[
\text{minimize } f(x) + I(x)
\]</span></p>
</div>
<div class="slide section level2">

<p>Problem: The <span class="math">\(I(x)\)</span> function doesn't play well with the methods we have for unconstrained optimization.</p>
<p>Solution: Make a nice (i.e. differentiable) approximation of <span class="math">\(I(x)\)</span>.</p>
<div class="incremental">
<p>The log-barrier function is our differentiable approximation: <span class="math">\[
\hat I_\mu(x) = \begin{cases}
- \mu \log(c - f_1(x)) &amp; f_1(x) - c \le 0\\
\infty &amp; f_1(x) - c &gt; 0
\end{cases}
\]</span></p>
<p>As <span class="math">\(\mu\)</span> gets closer to 0, <span class="math">\(\hat I_\mu\)</span> is a better approximation to <span class="math">\(I\)</span>.</p>
</div>
</div>
<div class="slide section level2">

<p>For example: if we want to enforce <span class="math">\(x &lt; 0\)</span> (<span class="math">\(f_1(x) = x, c = 0\)</span>), the log-barrier function would be <span class="math">\(-\mu \log(-x)\)</span></p>
<pre class="sourceCode r"><code class="sourceCode r">x =<span class="st"> </span><span class="kw">seq</span>(-<span class="dv">2</span>,<span class="dv">0</span>, <span class="dt">length.out =</span> <span class="dv">10000</span>)[-<span class="dv">10000</span>]
<span class="kw">plot</span>(-<span class="dv">2</span> *<span class="st"> </span><span class="kw">log</span>(-x) ~<span class="st"> </span>x, <span class="dt">type =</span> <span class="st">&#39;l&#39;</span>, <span class="dt">ylim =</span> <span class="kw">c</span>(-<span class="dv">3</span>, <span class="dv">10</span>), <span class="dt">ylab =</span> <span class="st">&quot;log barrier for x &lt; 0&quot;</span>, <span class="dt">lty =</span> <span class="dv">1</span>)
<span class="kw">points</span>(-<span class="dv">1</span> *<span class="st"> </span><span class="kw">log</span>(-x) ~<span class="st"> </span>x, <span class="dt">type =</span> <span class="st">&#39;l&#39;</span>, <span class="dt">lty =</span> <span class="dv">2</span>)
<span class="kw">points</span>(-.<span class="dv">5</span> *<span class="st"> </span><span class="kw">log</span>(-x) ~<span class="st"> </span>x, <span class="dt">type =</span> <span class="st">&#39;l&#39;</span>, <span class="dt">lty =</span> <span class="dv">3</span>)</code></pre>
<div class="figure">
<img src="lecture-19-fig/unnamed-chunk-1-1.png" />
</div>
</div>
<div id="interior-point-algorithm" class="slide section level2">
<h1>Interior Point algorithm</h1>
<p>Problem: <span class="math">\[
\begin{align*}
\text{minimize } &amp;f(x) \\
\text{subject to } &amp; f_i(x) \le c_i, \quad i = 1,\ldots, m
\end{align*}
\]</span></p>
<p>Start with a point <span class="math">\(x\)</span> in the feasible set, initial <span class="math">\(\mu\)</span></p>
<p>Repeat</p>
<ul>
<li><p>Minimize <span class="math">\(f(x) + \sum_{i=1}^m - \mu \log(c_i - f_i(x))\)</span>.</p></li>
<li><p>Reduce <span class="math">\(\mu\)</span>.</p></li>
</ul>
<p>Until the stopping criterion is reached.</p>
</div>
<div id="example-dice-again" class="slide section level2">
<h1>Example: dice again</h1>
<p>Suppose we think that our dice are biased in a certain way: <span class="math">\(\sum_{i=1}^3 p_i \ge .6\)</span>, and we also don't think any given <span class="math">\(p_i\)</span> is too small: <span class="math">\(p_i \ge .01\)</span>, <span class="math">\(i = 1,\ldots, 6\)</span>.</p>
<p>We can write the maximum likelihood problem subject to this constraint as</p>
<p><span class="math">\[
\begin{align*}
\text{maximize} \quad &amp; \ell(p_1, \ldots, p_n)\\
\text{subject to} \quad&amp; \sum_{i=1}^6 p_i = 1\\
&amp; \sum_{i=1}^3 p_i \ge .6\\
&amp; p_i \ge .01, \quad i = 1,\ldots, 6
\end{align*}
\]</span></p>
<p>We can incorporate the equality constraints as Lagrange multipliers: <span class="math">\[
\begin{align*}
\text{maximize} \quad &amp; \mathcal L(p_1, \ldots, p_n, \lambda)\\
\text{subject to} \quad&amp;  \sum_{i=1}^3 p_i \ge .6\\
&amp; p_i \ge .01, \quad i = 1,\ldots, 6
\end{align*}
\]</span> and try to use the barrier method to get the maximum likelihood estimate for the inequality-constrained problem.</p>
</div>
<div class="slide section level2">

<p>Define a function for the Lagrangian:</p>
<pre class="sourceCode r"><code class="sourceCode r">lagrangian =<span class="st"> </span>function(p_and_lambda, nvec) {
    p =<span class="st"> </span>p_and_lambda[<span class="dv">1</span>:<span class="dv">6</span>]
    lambda =<span class="st"> </span>p_and_lambda[<span class="dv">7</span>]
    if(<span class="kw">any</span>(p &lt;=<span class="st"> </span><span class="dv">0</span>)){
        <span class="kw">return</span>(-<span class="ot">Inf</span>)
    }
    L =<span class="st"> </span><span class="kw">sum</span>(nvec *<span class="st"> </span><span class="kw">log</span>(p)) -<span class="st"> </span>lambda  *<span class="st"> </span>(<span class="kw">sum</span>(p) -<span class="st"> </span><span class="dv">1</span>)
    <span class="kw">return</span>(L)
}
logbarrier =<span class="st"> </span>function(p, mu) {
    if(<span class="kw">sum</span>(p[<span class="dv">1</span>:<span class="dv">3</span>]) &gt;=<span class="st"> </span>.<span class="dv">6</span> &amp;&amp;<span class="st"> </span><span class="kw">all</span>(p &gt;=<span class="st"> </span>.<span class="dv">01</span>)) {
        <span class="kw">return</span>(-mu *<span class="st"> </span><span class="kw">log</span> (-.<span class="dv">6</span> +<span class="st"> </span><span class="kw">sum</span>(p[<span class="dv">1</span>:<span class="dv">3</span>])) -<span class="st"> </span>mu *<span class="st"> </span><span class="kw">sum</span>(<span class="kw">log</span>(-.<span class="dv">01</span> +<span class="st"> </span>p)))
    }
    <span class="kw">return</span>(<span class="ot">Inf</span>)
}
neg_lagrangian_and_logbarrier =<span class="st"> </span>function(p_and_lambda, mu, nvec) {
    p =<span class="st"> </span>p_and_lambda[<span class="dv">1</span>:<span class="dv">6</span>]
    -<span class="kw">lagrangian</span>(<span class="dt">p_and_lambda =</span> p_and_lambda, <span class="dt">nvec =</span> nvec) -<span class="kw">logbarrier</span>(<span class="dt">p =</span> p, <span class="dt">mu =</span> mu)
}</code></pre>
</div>
<div class="slide section level2">

<pre class="sourceCode r"><code class="sourceCode r">nvec =<span class="st"> </span><span class="kw">c</span>(<span class="dv">50</span>,<span class="dv">10</span>,<span class="dv">10</span>,<span class="dv">10</span>,<span class="dv">10</span>,<span class="dv">20</span>)
nvec /<span class="st"> </span><span class="kw">sum</span>(nvec)</code></pre>
<pre><code>## [1] 0.45454545 0.09090909 0.09090909 0.09090909 0.09090909 0.18181818</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">starting_p_and_lambda =<span class="st"> </span><span class="kw">c</span>(nvec /<span class="st"> </span><span class="kw">sum</span>(nvec),<span class="dv">100</span>)
mu =<span class="st"> </span>.<span class="dv">25</span>
## check that the starting values are valid
<span class="kw">logbarrier</span>(starting_p_and_lambda, <span class="dt">mu =</span> mu)</code></pre>
<pre><code>## [1] 2.834713</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">## optimize
o =<span class="st"> </span><span class="kw">optim</span>(starting_p_and_lambda,
    neg_lagrangian_and_logbarrier,
    <span class="dt">mu =</span> mu, <span class="dt">nvec =</span> nvec, <span class="dt">method =</span> <span class="st">&quot;SANN&quot;</span>)
<span class="kw">round</span>(o$par[<span class="dv">1</span>:<span class="dv">6</span>], <span class="dt">digits =</span> <span class="dv">3</span>)</code></pre>
<pre><code>## [1] 0.455 0.091 0.091 0.091 0.091 0.182</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sum</span>(o$par[<span class="dv">1</span>:<span class="dv">6</span>])</code></pre>
<pre><code>## [1] 1</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sum</span>(o$par[<span class="dv">1</span>:<span class="dv">3</span>])</code></pre>
<pre><code>## [1] 0.6363636</code></pre>
</div>
<div class="slide section level2">

<p>Try again with a different set of data values</p>
<pre class="sourceCode r"><code class="sourceCode r">nvec =<span class="st"> </span><span class="kw">c</span>(<span class="dv">35</span>,<span class="dv">10</span>,<span class="dv">10</span>,<span class="dv">10</span>,<span class="dv">10</span>,<span class="dv">20</span>)
nvec /<span class="st"> </span><span class="kw">sum</span>(nvec)</code></pre>
<pre><code>## [1] 0.3684211 0.1052632 0.1052632 0.1052632 0.1052632 0.2105263</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">current_p_and_lambda =<span class="st"> </span><span class="kw">c</span>(.<span class="dv">5</span>, .<span class="dv">1</span>, .<span class="dv">1</span>, .<span class="dv">1</span>, .<span class="dv">1</span>, .<span class="dv">1</span>,<span class="dv">100</span>)
mu =<span class="st"> </span><span class="dv">1</span>
alpha =<span class="st"> </span>.<span class="dv">5</span>
<span class="kw">logbarrier</span>(starting_p_and_lambda, <span class="dt">mu =</span> mu)</code></pre>
<pre><code>## [1] 11.33885</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">o =<span class="st"> </span><span class="kw">optim</span>(current_p_and_lambda,
    neg_lagrangian_and_logbarrier,
    <span class="dt">mu =</span> mu, <span class="dt">nvec =</span> nvec, <span class="dt">method =</span> <span class="st">&quot;Nelder-Mead&quot;</span>)
<span class="kw">cat</span>(<span class="kw">round</span>(o$par[<span class="dv">1</span>:<span class="dv">6</span>], <span class="dt">digits =</span> <span class="dv">3</span>), <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</code></pre>
<pre><code>## 0.341 0.122 0.137 0.068 0.099 0.148</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cat</span>(<span class="kw">sum</span>(o$par[<span class="dv">1</span>:<span class="dv">6</span>]), <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</code></pre>
<pre><code>## 0.9146866</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cat</span>(<span class="kw">sum</span>(o$par[<span class="dv">1</span>:<span class="dv">3</span>]), <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</code></pre>
<pre><code>## 0.6</code></pre>
</div>
<div id="summing-up" class="slide section level2">
<h1>Summing up</h1>
<ul>
<li><p>Lagrange multipliers to make equality-constrained problems into unconstrained problems.</p></li>
<li><p>Interior point methods solve the inequality-constrained problem by approximating a hard barrier with a soft one.</p></li>
<li><p>Idea overall is to change constrained problems into unconstrained ones, and then solve either the unconstrained problem or an approximation.</p></li>
</ul>
</div>
</body>
</html>
